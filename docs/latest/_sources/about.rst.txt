About `pysdg`
=============

**Python Synthetic Data Generator** (`pysdg`) is a package designed by the `Electronic Health Information Laboratory (EHIL) <https://www.ehealthinformation.ca/>`_ at the `CHEO Reserch Institute <https://www.ehealthinformation.ca/>`_.   The package is used to streamline the generation of synthetic data for clinical trials and support various types of analyses. It ensures consistency and robustness when handling different clinical trial datasets and generative models. The package acts as a wrapper for several reliable generative modeling implementations and includes two modules: `synth` and `privacy`.

The `synth` module generates single or multiple synthetic datasets (`synths`) from the input `raw` tabular `csv` dataset. The basic description of the real `raw` dataset shall be provided as a `json` file. In the `json` file, the user defines the indices of the categorical variables, continuous variables and others. To generate `synths`, the user can use any of the four available generators. Besides `synths`, the use is provided with clean `raw` datasets, namely `real` to allow for direct comparisons with `synths` The currently supported generators are listed in :ref:`usage`. 

The second module `privacy` calculates the membership disclosure risk and the attribution disclosure risk. The implementation of membership disclosure risk is based on the paper `Validating a membership disclosure metric for synthetic health data <https://pubmed.ncbi.nlm.nih.gov/36238080/>`_. The attribution disclosure risk uses `Replica` package (recently renamed to `Aetion Generate`). For teh calculation of attribution disclosure risk, a list of quasi identifiers shall be provided by the user in the `json` file described earlier. All connections to Replica package need proper credentials to be defined by the user in a separate `.env`` file. 

The following section illustrates `pysdg` core module to train models and generate synthetic data. We introduce the concepts of `raw`, `real` and `synth` datasets. Next, we provide an  overview of `pysdg`'s invaluable add-ons for calculating privacy risks and other functionalities. 

For any queries, please contact `Samer El Kababji <mailto:skababji@ehealthiformation.com>`_

Core functionality
******************

.. image:: images/pysd_basics.png
   :alt: `pysdg` Core functionality
   :align: center

`pysdg` includes the essential modules to carry out the functions illustrated by the dark background blocks in the above diagram. To generate one or multiple synthetic datasets, follow these steps in sequence: `Load`, `Train`, `Generate`, and `Unload`. The `raw` tabular dataset is loaded as a `csv` file. In addition, the dataset's user-defined metadata is loaded as a `json` file. Please refer to :ref:`usage` for the details of various keys in the `json` file. In return, the user gets the `real` dataset which is a clean version of the `raw` dataset. The `real` dataset can be used for direct comparison with the synthetic datasets. The load function also encodes the input `raw` data in a consistent manner and independent from the reading library. The encoded data is then used to train a selected generative model. Once trained, any required number of records and synthetic dataset versions are sampled form the generative model. The list of generated synthetic datasets are referred to as `synths`. 

Add-ons
*******

.. image:: images/pysd_addons_2.png
   :alt: `pysdg` Add-ons
   :align: center

As depicted in the figure above, additional functions are supported by pysdg (the dark background blocks in the figure).  This include the calculation of membership disclosure risk and the calculation of attribution disclosure risk. On the other hand, `pysdg` can be occasionally used to estimate the utility of the synthetic data as compared to the `real` data. For instance, a researcher may be interested to apply a linear regression analysis (the shaded boxes in the figure which is not part of `pysdg`) to the `real` dataset to check the effect size of a variable of interest. If the same analysis is applied to each synthetic version, the resulting multiple effect sizes can be correctly combined using the `combine estimates` function. Further, the utility resulting from the combined estimate can by calculated using the `compare estimates` function. This will return four utility metrics, namely, the decision agreement, the estimate agreement, the confidence interval overlap, and the standardized difference. The definition of each of these metrics is given in the function's :ref:`api_ref` section.





