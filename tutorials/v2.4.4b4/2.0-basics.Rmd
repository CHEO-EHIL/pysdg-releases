---
title: "pysdg in R"
output: html_document
---

# Tutorial: Basic Usage

**This tutorial demonstrates the core functions of pysdg.** It assumes that pysdg is already installed in a conda environment. The package reticulate is used to interface with Python. For respective installation instructions, please consult the pysdg documentation. 

**Core Functions** in pysdg include loading (real) training data, training a generator, generating synthetic data and unloading. This tutorial covers the basics in more detail.

## Step 1: Activate the Conda Environment
For the purpose of this tutorial, we assume that your conda environment was named `pysd_dev`, so the first step is to activate it. Note that Step 1 is *not* required if you are working with a pre-defined Docker image that has the required environment including its dependencies pre-installed.

```{r}
#library(reticulate) 
#use_condaenv("pysdg_dev", required = TRUE) 
```

## Step 2: Import Core Functions and Load R Libraries
Relevant functions are imported from pysdg:

* synth.load for standardized loading of datasets via pysdg, 
* synth.generate to generate a synthetic dataset

We import modules from the python package to handle them in R. Through reticulate, functions within the modules can be accessed via `$`. The R library jsonlite will be used for the purpose of this tutorial, it is not required for the functionality of pysdg.

```{r}
library(jsonlite)
synth.load <- reticulate::import("pysdg.synth.load")
synth.generate <- reticulate::import("pysdg.synth.generate") 
```

## Step 3: Define the Paths 
Typically, two files are necessary to generate synthetic data via pysdg: 

* first the `raw data` (or raw or real) tabular dataset in CSV format and 
* secondly the corresponding data `info` file in JSON format. 

The JSON file shall be manually created, it provides metadata for the dataset and has to include several mandatory keys (see Step 5). Below are the paths to both files:

```{r}
current_file_path <- rstudioapi::getActiveDocumentContext()$path
raw_data_path <- paste0(dirname(current_file_path), "/raw_data.csv")
raw_info_path <- paste0(dirname(current_file_path), "/raw_info.json")
```

## Step 4: Explore the Real (Training) Data
First, let us read in the `raw data` with the base R function and take a look to the first few rows.

```{r}
raw_data <- read.csv(raw_data_path)
head(raw_data, 10)
```

Let us also take a look to the datatypes as interpreted by the default settings of the base R read in function. We can see that it does, for example, not recognize `event_dt` as datetime. Clearly, these datatypes can vary depending on the function used for reading the CSV file. Loading the data with the pysdg function can ensure that datatypes are in line with provided metadata. 

```{r}
str(raw_data)
```
This means, we have to define the datatypes for all the variables to eliminate the dependency on the function used to read the CSV file. To simplify things, pysdg identifies four basic data types: 

* categorical (`cat`), 
* continuous (`cnt`) , 
* discrete (`dscrt`) and 
* datatime (`datetime`).  

Please note that **the categorical variable does not have to be a string; it can also be represented by numeric values**. In the JSON file, we list all the indices of the variables under the right data type. 

We can also see that the `raw data` above includes several representations of missing values, e.g.  `NA`, `NAN, NaT` and `<NA>`. We need to define that in the metadata JSON file.

## Step 5: Explore the Metadata
Let us take a look to the JSON file that we created earlier for the purpose of this tutorial.

```{r}
raw_info <- fromJSON(raw_info_path)
raw_info
```

<<<<<<< HEAD
As you see above, the dataset is given the name `tutorial_data`. This can be any name. For the time being, we will define empty lists corresponding to the keys `nct_nos`, `id_idx`, `quasi_idxs`. We also keep `h0_value` set to a list of single element which is zero. Let us focus on the remaining keys. The list corresponding to the key `cat_idxs` includes  the indexes of the categorical variables as defined by the user. For instance, the first variable (index 0) in the `raw data`, namely, `outc_cod_0` is defined as categorical, while the third variable (index 2), namely `wt` is defined as continuous.  

We also note that all the occurring missing value representations are listed under `miss_vals`. Adding more representations not existing in the `raw data` is allowed and will have no impact. It is always advisable to include `nan`, `NA` and `''`.

Before loading both the CSV file and its corresponding  JSON file, we need to define a generator object. We pass the name of the desired generator as an argument. You can refer to `pysdg` documentation for a list of the names of available generators. In this tutorial, will use the [bayesian network generator from Synchcity](https://synthcity.readthedocs.io/en/latest/generated/synthcity.plugins.generic.plugin_bayesian_network.html), namely, `synthcity_bayesian_network`.

```{r}
gen <- synth.generate$Generator("synthcity/bayesian_network")
```

Now we will load both the `raw data` path and its user-defined `raw info` path using the load method. In return, we will get back a clean `real` data that we can use in our downstream analysis.

```{r}
real <- gen$load( raw_data_path, raw_info_path) 
```

The `load` method gives you the option to load the raw dataframe object rather than the raw data path e.g.

```{r}
real <- gen$load( raw_data, raw_info_path) 
```

The clean `real data` enforces the data types as per the input `raw info` json file. Lets take a look to that as compared to the data types in the `raw data`. You can see below that all data types match what was defined in the `raw info` jon file. 

```{r}
sapply(real, class)
```

Moreover, all the variables in the `real` data will hold missing value representations conforming to their datatype. Let us take a look to teh first rows of `real` data as compared to `raw data`. It is imperative that if the `real` is saved to a CSV file, all missing values will hold a unified representation. 

```{r}
kable(head(real,10))
```
=======
As you see above, the dataset is given the name `tutorial_data`. This can be any name. For the time being, we will not specify the keys `nct_nos` and `id_idx`, but provide empty lists. We also keep `h0_value` set to a list of single element which is zero. `quasi_idxs` is relevant to quantify disclosure vulnerability (see tutorial on membership disclosure vulnerability). 

Let us focus on the remaining keys. The list corresponding to the key `cat_idxs` includes  the indices of the categorical variables as defined by the user. Note that the indices are defined for the Python Pandas library. **In contrast to R, indices start with 0**. For instance, the first variable (index 0) in the `raw data`, namely, `outc_cod_0` is defined as categorical: this is the outcome variable with 7 categories. The third variable (index 2), namely `wt`, is defined as continuous: this is the weight.  

We also note that all the occurring missing value representations are listed under `miss_vals`. Adding more representations not existing in the `raw data` is allowed and will have no impact. It is always advisable to include `nan`, `NA` and `''`.

## Step 6: Create a Generator Object
Before loading both the CSV file and its corresponding  JSON file, we need to define a generator object. We pass the name of the desired generator as an argument. You can choose between different generators. We use Bayesian Network as example. Check out the most recent documentation: we extend the selection of generators regularly. In this tutorial, will use the [bayesian network generator from Synchcity](https://synthcity.readthedocs.io/en/latest/generated/synthcity.plugins.generic.plugin_bayesian_network.html), namely, `synthcity_bayesian_network`.

```{r}
gen <- synth.generate$Generator("synthcity_bayesian_network")
```

## Step 7: Load (Real) Training Data With Provided Metadata
Now we will load both the `raw data` path and its user-defined `raw info` path using the load method. This loading process converts the datatypes in the dataset according to the specifications in the json file.

```{r, message = F, warning = F}
train_data <- gen$load(raw_data_path, raw_info_path) 
```

### Alternative: Load (Real) Training Data Without Path
The `load` method gives you the option to load a dataset (i.e., `raw data`) directly into the generator object rather than requiring a path.

```{r, message = F, warning = F}
train_data <- gen$load(raw_data, raw_info_path) 
```

## Step 8: Explore the Loaded Training Data
The clean `train data` enforces the data types as per the input `raw info` json file. Lets take a look to that as compared to the data types in the `train data`. You can see below that all datatypes match what was defined in the `raw info` json file. 

```{r}
str(train_data)
```

Moreover, all the variables in the `train data` data will hold declared missing value representations as specified in the json file. Let us take a look to the first rows of `raw data` as compared to `train data`. 
```{r}
head(raw_data, 10)
```

```{r}
head(train_data,10)
```

While the transformation of missing values will be preserved when the `train data` is saved to a CSV file, this may not be true for the datatypes. Again, depending on the function used to read in CSV files, datatypes can change. Loading via pysdg can ensure consistency.

## Step 9: Explore the Loaded Metadata
>>>>>>> 3c546e779121d07c2befc8c51d736ad57977a88e
We can further explore what happens with the input `raw info` file. Let us retrieve the `info` from our `gen` object. As you see below, the variable indexes are converted into variable names.

```{r}
gen$real_info
```

<<<<<<< HEAD
The `load` method encodes the `real` data to be used for training the desired generator. Let us take a look to the `encoded real` data frame.

```{r}
kable(head(gen$enc_real,10))
```

After loading the data, we can start training the desired generator. 

```{r}
gen$train() 
```

Once trained, the model can be used to generate the required number of records and synthetic datasets. In the following code line below, we are generating two synthetic datasets, each with the same number of records of the real data set. 

```{r}
gen$gen(num_rows=nrow(real), num_synths=2) 
```

The generated synthetic datasets are both encoded. For instance, we can check the first 10 records of the first synthetic dataset using:

```{r}
kable(head(gen$enc_synths[[1]],10))
```

The synthetic datasets need to be decoded and we can use `unload` method as the final step to retrieve the list of the generated synthetic data sets, which is called below `synths`.

```{r}
synths<-gen$unload()
=======
## Step 10: Explore the Encoded Training Data
The `load` method encodes the `raw` data` to be used for training the desired generator. This involves several pre-processing steps to make the data ready for the generator. Let us take a look to the `encoded real` data frame.

```{r}
head(gen$enc_real,10)
```

## Step 11: Train the Generator
The generator is trained on the loaded (real) training dataset.  

```{r, message = F, warning = F}
gen$train() 
```

## Step 12: Generate Synthetic Data
Once trained, the model can be used to generate any required number of records and synthetic datasets. In the following code line below, we are generating synthetic datasets with the same number of records of the training dataset. Typically, 10 synthetic datasets are generated to account for the stochasticity of the process. The number of rows (num_rows) typically match the number of rows in the real data.

```{r}
gen$gen(num_rows=nrow(train_data), num_synths=10) 
```

## Step 13: Explore the Encoded Synthetic Data
The generated synthetic datasets are stored as a list within the generator object. They are also encoded. For instance, we can check the first 10 records of the first synthetic dataset using:

```{r}
head(gen$enc_synths[[1]],10)
```

## Step 14: Decode the Synthetic Datasets
The synthetic datasets need to be decoded and we can use `unload` method as the final step to retrieve the list of the generated synthetic data sets, which is called below `synths`. We then index the first dataset.

```{r, message = F, warning = F}
synths <- gen$unload()
synth <- synths[[1]]
>>>>>>> 3c546e779121d07c2befc8c51d736ad57977a88e
```

Let us check the first 10 records of the first synthetic data set.

```{r}
<<<<<<< HEAD
kable(head(synths[[1]],10))
```

Clearly, the final generated `synthetic` data sets have exactly the same data types and column names and arrangements of the `real` data set.

```{r}
sapply(synths[[1]], class)
```


=======
head(synth,10)
```

Clearly, the final generated `synthetic data` has exactly the same structure as the `train data`. The unloaded synthetic datasets can then be evaluated against the loaded training data.

```{r}
str(synth)
```
>>>>>>> 3c546e779121d07c2befc8c51d736ad57977a88e
