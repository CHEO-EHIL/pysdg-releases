{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This tutorial demonstrates the core functions of pysdg.** It assumes that pysdg is already installed in a Conda environment, the environment has been activated from the shell, and this notebook is being run within that activated environment. For detailed instructions, please refer to the \"pysdg\" documentation.\n",
    "\n",
    "The following cell sets the working directory to the location of this notebook. It is assumed that all files accessed by this notebook are stored in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "current_dir = Path().resolve()\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core functions in pysdg include: loading, training, generating and unloading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary packages and apply the proper settings for prettier display of both Pandas data frames and Python dictionaries. The last line below imports the  Generator class from `pysdg` synth module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_colwidth', 10)  \n",
    "pd.set_option('display.width', 1000)  \n",
    "\n",
    "import json \n",
    "from IPython.display import JSON\n",
    "\n",
    "from pysdg.synth.generate import Generator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two files are necessary to be loaded into pysdg: first the `raw` tabular dataset in CSV format and secondly the corresponding data `info` file in JSON format. The JSON file shall be manually created and it has to include several mandatory keys. Below are the paths to both files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path='raw_data.csv'\n",
    "raw_info_path='raw_info.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us take a look to the first few rows of the `raw data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv(raw_data_path)\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also take a look to the data types as interpreted by the default settings of pandas. Clearly, these data types can vary depending on the library used for reading the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `raw data` above includes several representations of missing values, i.e.  `NA`, `NAN, NaT` and `<NA>`. We need to define that in the metadata JSON file. \n",
    "\n",
    "We also need to define the data types for all the variables to eliminate the dependency on the library used to read the CSV file. To simplify things, pysdg identifies four basic data types: categorical (`cat`), continuous (`cnt`) , discrete (`dscrt`) and datatime (`datetime`).  Please note that **the categorical variable can be either numbers or alphabets**. In the JSON file, we list all the indexes of the variables under the right data type. Let us take a look to the JSON file that we created earlier for the purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_info_path,\"r\") as f:\n",
    "    raw_info=json.load(f)\n",
    "    \n",
    "JSON(raw_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, the dataset is given the name `tutorial_data`. This can be any name. For the time being, we will define empty lists corresponding to the keys `nct_nos`, `id_idx`, `quasi_idxs`. We also keep `h0_value` set to a list of single element which is zero. Let us focus on the remaining keys. The list corresponding to the key `cat_idxs` includes  the indexes of the categorical variables as defined by the user. For instance, the first variable (index 0) in the `raw data`, namely, `outc_cod_0` is defined as categorical, while the third variable (index 2), namely `wt` is defined as continuous.  \n",
    "\n",
    "We also note that all the occurring missing value representations are listed under `miss_vals`. Adding more representations not existing in the `raw data` is allowed and will have no impact. It is always advisable to include `nan`, `NA` and `''`.\n",
    "\n",
    "Before loading both the CSV file and its corresponding  JSON file, we need to define a generator object. We pass the name of the desired generator as an argument. You can refer to `pysdg` documentation for a list of the names of available generators. In this tutorial, will use the [bayesian network generator from Synchcity](https://synthcity.readthedocs.io/en/latest/generated/synthcity.plugins.generic.plugin_bayesian_network.html), namely, `synthcity_bayesian_network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen=Generator(\"synthcity_bayesian_network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load both the `raw data` path and its user-defined `raw info` path using the load method. In return, we will get back a clean `real` data that we can use in our downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real=gen.load(raw_data_path, raw_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load` method gives you the option to load the raw dataframe object rather than the raw data path e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real=gen.load(raw_data, raw_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean `real data` enforces the data types as per the input `raw info` json file. Let's take a look to that as compared to the data types in the `raw data`. You can see below that all data types match what was defined in the `raw info` jon file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, all the variables in the `real` data will hold missing value representations conforming to their datatype. Let us take a look to teh first rows of `real` data as compared to `raw data`. It is imperative that if the `real` is saved to a CSV file, all missing values will hold a unified representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further explore what happens with the input `raw info` file. Let us retrieve the `info` from our `gen` object. As you see below, the variable indexes are converted into variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(gen.real_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load` method encodes the `real` data to be used for training the desired generator. Let us take a look to the `encoded real` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.enc_real.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we can start training the desired generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, the model can be used to generate the required number of records and synthetic datasets. In the following code line below, we are generating two synthetic datasets, each with the same number of records of the real data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.gen(num_rows=len(real), num_synths=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated synthetic datasets are both encoded. For instance, we can check the first 10 records of the first synthetic dataset using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.enc_synths[0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic datasets need to be decoded and we can use `unload` method as the final step to retrieve the list of the generated synthetic data sets, which is called below `synths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synths=gen.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the first 10 records of the first synthetic data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synths[0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the final generated `synthetic` data sets have exactly the same data types and column names and arrangements of the `real` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synths[0].dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysdg_edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
